---
title: DSAN 5500 Final Project Job Position ETL Pipeline
subtitle: A DSAN 5500
author: Jen Guo and Wendy Hu
date: last-modified
date-format: long
format:
  html:
    toc: true
    code-copy: true
    code-overflow: wrap
    mainfont: Atkinson Hyperlegible
    code-annotations: hover
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
  freeze: auto 
jupyter: python3
---

# Set Up API Key

```{python}
#load needed packages
import json
import os
import requests

#retrieve API key from stored json file
try:
    with open('Jen_api.json') as f:
        keys = json.load(f)
    
        #store API key and app_id
        APP_ID = keys['app_id']
        API_KEY = keys['api_key']
        
#handle errors if json file not found and if api key is not valid
except FileNotFoundError:
    print("The file storing api key was not found.")
except KeyError as e:
    print(f"Missing key in json file: {e}")
except json.JSONDecodeError:
    print("Error reading json file.")
```

# Test 1 extract, transform, and print out job postions

```{python}
# Define your search parameters
# params = {
#     'app_id': APP_ID,
#     'app_key': API_KEY,
#     'what': 'data scientist',          # Job title or keywords
#     'where': 'Washington, DC',         # Location
#     'results_per_page': 2,             # Number of results per page
#     # 'sort_by': 'date'                  # Sort by most recent
# }
```

```{python}
#define url site on Adzuna to scrape job position posts
# url = 'https://api.adzuna.com/v1/api/jobs/us/search/1'

```

```{python}
# Make the request
# response = requests.get(url, params=params)

```

```{python}
# Check response
# if response.status_code == 200:
#     data = response.json()
#     for job in data.get('results', []):
#         print('ðŸ§¾ Job Title:', job.get('title'))
#         print('ðŸ¢ Company:', job.get('company', {}).get('display_name'))
#         print('ðŸ“ Location:', job.get('location', {}).get('display_name'))
#         print('ðŸ“ Description:', job.get('description')[:150], '...')
#         print('ðŸ”— URL:', job.get('redirect_url'))
#         print('-' * 80)
# else:
#     print('Failed to retrieve jobs:', response.status_code)
#     print(response.text)
```

# Test 2 with BaseModel class ETL

# Extract Step

```{python}
#install needed packages
from pprint import pprint
from typing import List, Optional, Union
import string

#from bs4 import BeautifulSoup
from pydantic import BaseModel, field_validator, HttpUrl, ValidationError, FilePath

#define GetJobs class
class GetJobs(BaseModel):
  #intialize variables in GetJobs class
  title: str
  company: str
  location: str
  salary: Optional[Union[int, float, str]]
  hours: str
  degree: str
  description: str
  responsibilities: str
  skills: str
  about_url: HttpUrl

  #validator to clean the extracted salary to replace "$" and "," symbols to convert to integer
  @field_validator('salary')
  @classmethod
  def clean_salary(cls, value):
      #if the extracted salary is integer or float value then return the salary value
      if isinstance(value,(int, float)):
          return value
      #if the extracted salary is string type then remove the "$" and the "," symbols
      if isinstance(value, str):
          cleaned_sal = value.replace("$", "").replace(",", "").strip()
          #handle errors for string type extracted salary
          try:
              return int(float(cleaned_sal))
          except ValueError:
              return "Not specified"
          return "Not specified"
 
      
  #validator for extracted location to replace "," symbol
  @field_validator('location')
  @classmethod
  def clean_location(cls, value: str):
    return value.replace("â€œ","").replace("â€","")


#define API request parameters
params = {
    'app_id': APP_ID,                  # app ID
    'app_key': API_KEY,                # API key
    'what': 'data scientist',          # Job title or keywords
    'where': 'Washington, DC',         # Location
    'results_per_page': 1,             # Number of results per page
    'sort_by': 'date'                  # Sort by most recent
}

#create extract_jobs function to extract job posts
def extract_jobs(job_url: HttpUrl) -> List[GetJobs]:
  response = requests.get(job_url, params=params) #request from the Adzuna job post page using the request library
  #print error message if could not fetch job posts from page
  if response.status_code != 200:
      raise Exception(f"Failed to fetch jobs: {response.status_code}")

  #get job posts in json format
  jobs_data = response.json()
  #initialize empty list to store extracted job posts
  extracted_jobs = []

  #for each of the extracted job posts
  for job in jobs_data.get("results", []):
      try:
          job_post = GetJobs( #call GetJobs class
              title = job.get("title", "No title"), #extract the job title of the job position post
              company = job.get("company", {}).get("display_name", "Unknown"), #extract the company name of the job position post
              location = job.get("location", {}).get("display_name", "Unknown"), #extract the job location of the job position post
              salary = job.get("salary", {}).get("display_name","Not specified"), #extract the job salary information of the job position post
              hours = job.get("hours", {}).get("display_name", "Unknown"), #extract employment hours from job position post
              degree = job.get("degree", {}).get("display_name", "Unknown"), #extract the work hours ex. Full-time, part time for the job position post
              description = job.get("description", "")[:250], #extract the job description of the job post
              responsibilities = job.get("responsibilities", {}).get("display_name", "Unknown"), #extract the responsibilities
              skills = job.get("requirements", {}).get("display_name", "Unknown"), #extract the required skills of the job position post
              about_url = job.get("redirect_url")) #extract the url page of job post
          
          extracted_jobs.append(job_post) #add extracted job to list
      except ValidationError as e: #raise error if job wasn't able to get extracted
          print(f"Validation error: {e}")
          
  return extracted_jobs #return extracted job posts with the requested parameters
          
          
  # soup = BeautifulSoup(response.text, 'html.parser') #use BeautifulSoup to extract information from the Adzuna job post page
  # job_title = soup.find('title', class_='title').text #extract the job title of the job position post
  # company_name = soup.find('company', class_ = 'company').text #extract the company name of the job position post           
  # job_location = soup.find('location', class_ = 'location') #extract the job location of the job position post
  # job_salary = soup.find('salary', class_ = 'salary') #extract the job salary information of the job position post
  # job_description = soup.find(('description')[:150], '...', class_ = 'description') #extract the job description of the job post
    

#keep this code but moved down atter transformation step
#create function to define url to extract job posts
# def scrape_job(
#     job_url: str = "https://api.adzuna.com/v1/api/jobs/us/search/1" #extract job posts from this Adzuna url
# ):
#   extracted_jobs = extract_jobs(job_url) #call the extract_jobs function
#   return extracted_jobs #return the extracted job posts for the defined url

# scrape_job() #call the scrape_job function
```

# Transformation Step

```{python}
#create function to transform extract job info into a friendly email structure
def transform_extracted_jobs(job_obj: List[GetJobs]) -> str:
    #if no job post extracted then return error message
    if not job_obj:
        return "No job posts found."

    #set text for email body
    email_body = "Your Daily Job Digest: Data Scientist roles in Washington, DC*\n\n"

    #iterate over extracted job to be displayed in the following format
    for i, job in enumerate(job_obj, start=1):
        job_entry = (
            f"Title {i}: {job.title}**\n"                   #print job title
            f"Company: {job.company}\n"                     #print company name
            f"Location: {job.location}\n"                   #print location of company
            f"Salary: {job.salary}\n"                       #print job salary
            f"Employment type: {job.hours}\n"               #print job type in terms of full-time. part-time, internship
            f"Degree requirement: {job.degree}\n"           #print education requirement for job post
            f"Description summary: {job.description}\n"     #print job description
            f"Responsibilities: {job.responsibilities}\n"   #print job responsibilites
            f"Technical skills: {job.skills}\n"             #print required technical skills needed for job postion
            f"More info url: {job.about_url})\n\n"          #print url of the job post
        )
        email_body += job_entry #append the job_entry to email_body
    return email_body #return the email_body
    
        
```

# Load Step

```{python}
#create function to load the extracted job posts in email format into a json file as test 
def load_jobs_to_file(job_str: str, load_filepath: str) -> None:
    with open(load_filepath, 'a') as file:
        file.write(job_str + '\n')
```

```{python}
#create function to define url to extract job posts
def scrape_job(
    job_url: str = "https://api.adzuna.com/v1/api/jobs/us/search/1", #extract job posts from this Adzuna url
    jobs_filename: str = "extracted_jobs.json"
):
  extracted_jobs = extract_jobs(job_url) #call the extract_jobs function

  #add the transformatiom_extracted_jobs function step
  email_template = transform_extracted_jobs(extracted_jobs)
  #print out the email_friendly version of extracted jobs in email template
  print(email_template)

  #file_result = load_jobs_to_file(email_template, jobs_filename)
  #return(file_result) #return extracted job posts in the json file output
  load_jobs_to_file(email_template, jobs_filename)

scrape_job() #call the scrape_job function
```

#